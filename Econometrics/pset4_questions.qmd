---
title: "Homework 4"
author: "Ashutosh Ekade"
format: html
---

## Question 1: Linear Regression with Normal Errors

Load the `BostonHousing2` dataset from the `mlbench` package, which has data on 506 census tracts from the 1970's. Assume the model is $Y = \beta_0 + \beta_1X_1 + \beta_2X_2 + \beta_3X_3 + e$ with $e \sim N(0,\sigma^2)$, observations are independent, and where $Y$ denotes `medv` (the median value of homes in 1000's of dollars), $X_1$ denotes `rm` (the average number of rooms per home), $X_2$ denotes `age` (the proportion of older homes), and $X_3$ denotes `crim` (the crime rate in the area).  See `?mlbench::BostonHousing2` for definitions of these variables.

```{r}
#install.packages('mlbench')
library(mlbench)
data('BostonHousing2')
```


#### 1.a) Write, mathematically, the joint log-likelihood function.  What link function (or inverse link function) is required to "connect" $\mu_i$ to $x_i'\beta$?

$$

\begin{align}
& \\l(\beta, \sigma^2) 
&= \sum_{i=1}^{n} \log f(Y_i | Y_i, \beta, \sigma^2) \\
&= -\frac{n}{2} \log(2\pi) - \frac{n}{2} \log(\sigma^2) -\frac{1}{2\sigma^2} \sum_{i=1}^{n} (Y_i - X'_i \beta)^2 \\
&= -\frac{n}{2} \log(2\pi) - \frac{n}{2} \log(\sigma^2) -\frac{1}{2\sigma^2} \sum_{i=1}^{n} (Y - X\beta)'(Y - X\beta) \\
\end{align}

$$


#### 1.b) Write an `R` function to calculate the log-likelihood function from 1a above.  Your function should take 3 arguments: (1) $\theta = (\beta, \sigma^2)$ a vector of all (five) parameters, (2) an $n \times k$ matrix $X$, and (3) a vector or $n \times 1$ matrix $y$.

```{r}
log_likelihood <- function(theta, X, y) {
      beta <- theta[1:4]
      sigma_sq <- theta[5]
      # log-likelihood
      n <- length(y)
      residuals <- y - X %*% beta
      log_likelihood_value <- -n/2 * log(2 * pi) - n/2 * log(sigma_sq) - 1/(2 * sigma_sq) * sum(residuals^2)
    return(log_likelihood_value)
  }
```


#### 1.c) Use `optim()` and your function from 1b to find $\hat{\beta}_\text{MLE}$ and $\hat{\sigma}^2_\text{MLE}$ as well as their standard errors. You may find it helpful to initialize your search at 0 for the $\beta$ parameters and $\text{var}(y)$ for $\sigma^2$.  You will likely find some differences in your standard errors compared to the output from `lm()` in 1d below.  This is because `lm()` uses analytical expressions for the standard errors, whereas `optim()` uses numerical approximations to the Hessian matrix. 

```{r}
set.seed(1234)
X <- cbind(1,BostonHousing2$rm,BostonHousing2$age,BostonHousing2$crim)
Y <- matrix(BostonHousing2$medv,ncol=1)
initial_parameters <- c(1,1,1,1,var(Y))
#optimization function is called
result <- optim(par = initial_parameters, fn = log_likelihood,  X=X, y=Y,control = list(fnscale = -1),hessian = TRUE,method = "BFGS")
print(result) 
#MLE estimates
beta_mle <- result$par[1:4]
sigma_square_mle <- result$par[5]
std_error <- sqrt(-diag(solve(result$hessian)))
print("Beta MLE")
print(beta_mle)
print("Sigma Square MLE")
print(sigma_square_mle)
print("Standard Error")
print(std_error)
```


#### 1.d) Use `lm()` and `summary()` to check your work in 1c above.

```{r}
summary(lm(BostonHousing2$medv ~ BostonHousing2$rm+BostonHousing2$age+BostonHousing2$crim))
```


#### 1.e) Do the signs on the 3 estimated slope coefficients make sense?  Why or why not?

The estimated slope coefficients align with expectations for the median home value determination. As generally observed, the number of rooms per home shows a positive relationship, while both age and crime rate exhibit inverse correlations with home values. Older houses typically depreciate in value over time, reflecting a negative correlation, while higher crime rates often decrease the desirability of an area, leading to lower demand and subsequently lower home values. Therefore, the estimated coefficients in the model are anticipated to be negative, reflecting the adverse effects of house age and crime rate on the estimated median home value.


#### 1.f) In 1c, `optim()` returned the hessian matrix, which enabled you to calculate the standard errors of the MLEs. In 1d, `lm()` provided the standard errors from analytical expressions we derived in class.  Suppose, however, that you were unable to calculate the standard errors and needed to use a bootstrap to estimate them.  Write `R` code to perform a bootstrap to estimate the standard errors of the 3 $\hat{\beta}_\text{MLE}$ parameters.  Each time through the loop, you may use `lm()` and `coef()` if you would like, or you may calculate $\hat{\beta}$ directly from matrices; you may **not** use the `boot()` function from the `boot` package.

```{r}
set.seed(1234)
B <- 1000
n <- nrow(X)
res <- matrix(NA_real_, nrow=B, ncol=ncol(X))
X_test <- X[,c(2,3,4)]

for(b in 1:B) {
  draws <- sample(1:n, size=n, replace=T)
  Y_boot <- Y[draws,]
  X_boot <- X_test[draws,]
  res[b,] <- lm(Y_boot ~ X_boot)$coef
}

serr <- apply(res, 2, function(x) sqrt(var(x)))

print("The standard errors estimated from the bootstrap are:")
serr[cbind(2,3,4)]
```


#### 1.g) Use your estimates and standard errors from 1c above to test whether the 3 $\beta$ slope coefficients are each (separately) statistically significantly different from zero at a 95% confidence level. Check your results against the output in 1d above.

```{r}
alpha <- 0.05
critical_value <- qt(1-alpha/2,n-3)
beta_stat <- beta_mle/std_error[1:4]

print("Beta Statistics")
beta_stat

critical_value <- qt(1 - alpha / 2, n-3)
print("Critical Value")
print(critical_value)

print("All the 3 slope coefficients are statistically significant (at 95% confidence level) because absolute value of each t-stat is greater than critical value.")
```


\newpage

## Question 2:  A Poisson Model for Count Data

Load the `trading_behavior` dataset. 

The data provides 200 observations on equity trading behavior of Anderson students. `id` is an anonymized identifier for the student.  `numtrades` is the median weekly number trades made by each student during the Fall quarter.  `program` indicates whether the student is in the MSBA (1), MBA (2), or MFE (3) program (note that you may need to store this variable as a factor or convert it to a set of dummy variables when using it to fit a statistical model).  `finlittest` is the students' scores on a financial literacy test taken before entering their graduate program (higher scores indicate higher financial "literacy"). 

Assume you want to model the number of trades as a function of graduate program (where $\mathbbm{1}$ is an indicator function) and financial literacy:

$$ y_i \sim \text{Pois}(\mu_i) $$

$$ \log \mu_i = \beta_0 + \beta_1\mathbbm{1}(MBA) + \beta_2\mathbbm{1}(MFE) + \beta_3 \text{finlittest} $$

```{r}
trading_behavior <- read.csv("trading_behavior.csv")
head(trading_behavior)
```


#### 2.a) A Poisson density for random variable $Y$ with parameter $\mu$ is $f(y|\mu) = \exp(-\mu)\mu^y/y!$. Suppose we let each $Y_i$ have it's own parameter $\mu_i$ with link function $\log(\cdot)$: specifically, $\log(\mu_i) = x_i'\beta$.  Assume the data are sampled independently.  Write, mathematically, the joint log-likelihood function.

$$

\begin{align}
& f(y|\mu) = \frac{e^{-\mu} \mu^y}{y!} \\
& \log(\mu_i) = \beta_0 + \beta_1 \times 1(\text{MBA}) + \beta_2 \times 1(\text{MFE}) + \beta_3 \times \text{finlittest} \\
& \log f(y_i | \mu_i) = \log \left( \frac{e^{-\mu_i} \mu_i^{y_i}}{y_i!} \right) = -\mu_i + y_i \log(\mu_i) - \log(y_i!) \\
& \ell(\beta) = \sum_{i=1}^{n} \left[ -\mu_i + y_i \log(\mu_i) - \log(y_i!) \right] \\
& \ell(\beta) = \sum_{i=1}^{n} \left[ -\mu_i + y_i \log(\mu_i) - \log(\Gamma(y_i + 1)) \right] \\
& \mu_i = e^{(\beta_0 + \beta_1 \times 1(\text{MBA}_i) + \beta_2 \times 1(\text{MFE}_i) + \beta_3 \times \text{finlittest}_i)}
\end{align}

$$


#### 2.b) Write an `R` function to calculate the log-likelihood function from 2a above.  Your function should take 3 arguments: (1) a vector $\beta$ of all (four) parameters, (2) an $n \times k$ matrix $X$, and (3) a vector or $n \times 1$ matrix $y$.

```{r}
trading_behavior$program <- relevel(factor(trading_behavior$program), ref = 'MSBA')

x <- model.matrix(~ program + finlittest, data = trading_behavior)
y <- as.matrix(trading_behavior$numtrades)
initial_beta <- as.vector(rep(0, ncol(x)))

loglik_poisson <- function(beta, x, y) {
    
    predicted_values <- x %*% beta
    
    mu <- exp(predicted_values)
    
    llp <- sum(-mu + y * predicted_values - lgamma(y + 1))
    
    return(llp)
    }

print(loglik_poisson(initial_beta, x, y))
```


#### 2.c) Use `optim()` and your function from 1b to find $\hat{\beta}_\text{MLE}$ as well as their standard errors. 

```{r}
initial_beta <- as.vector(rep(0, ncol(x)))

set.seed(1234)
result <- optim(par=initial_beta, 
                fn=loglik_poisson, x=x, 
                y=y, method = "BFGS", 
                control=list(fnscale=-1),
                hessian = TRUE)

beta_mle_poisson <- result$par
std_errors_mle_poisson <- sqrt(-diag(solve(result$hessian)))

print(beta_mle_poisson)
print(std_errors_mle_poisson)
```


#### 2.d) Fit the model using `glm()`.  Compare your results to the output from `optim` in question 2c above.

```{r}
glm(y ~ x - 1, data=trading_behavior, family = poisson())
```


#### 2.e) The "analog" to the F-test from linear regression is the Likelihood Ratio Test.  The Likelihood Ratio test statistic is calculated as:

$$ LR_n = 2 \times [ \ell_n(\hat{\theta}) - \ell_n(\tilde{\theta})] $$

#### where $\ell_n(\cdot)$ is the log likelihood function, $\hat{\theta}$ is the MLE, and $\tilde{\theta}$ is a constrained parameter vector (e.g., suppose a Null Hypothesis is that $\theta_2=0$ & $\theta_3=0$).  The Likelihood Ratio test statistic $LR_n$ has an asymptotic chi-squared distribution with $k$ degrees of freedom (i.e., $\chi^2_k$ where $k$ is the length of the $\theta$ vector).

#### Test the joint hypothesis that $\beta_2=0$ & $\beta_3=0$ at the 95% confidence level using a Likelihood Ratio test. Specifically, use your log-likelihood function from 2b above and your parameter estimates from 2c above to calculate $\ell_n(\hat{\beta_\text{MLE}})$.  Then replace $\beta_2$ and $\beta_3$ with their hypothesized values and re-calculate the log-likehood (ie, $\ell_n([\hat{\beta}_1,0,0,\hat{\beta}_4])$.  Next, compute $LR_n$ and compare the value to the cut-off of a chi-squared distribution with 4 degrees of freedom to assess whether or not you reject the Null Hypothesis.

```{r}
loglik_mle <- loglik_poisson(beta_mle_poisson, x, y)

beta_constrained <- beta_mle_poisson
beta_constrained[2:3] <- 0

loglik_constrained <- loglik_poisson(beta_constrained, x, y)

# Log likelihood ratio test statistic calculation
LR_t <- 2 * (loglik_mle - loglik_constrained)

# Degrees of freedom = number of constrained parameters
df <- 2

# Critical value from chi-squared distribution
chi_squared_critical <- qchisq(0.95, df)

cat("Likelihood Ratio Test Statistic:", LR_t, "\n")
cat("Chi-squared critical value at 95% confidence level:", chi_squared_critical, "\n")

# Decision
if (abs(LR_t) > chi_squared_critical) {
  cat("Reject the null hypothesis: beta_2 and beta_3 is non-zero.")
  } else {
  cat("Fail to reject the null hypothesis: beta_2 and beta_3 may both be zero.")
    }
```

We reject the null hypothesis which means that both beta_2 and beta_3 are zero as the test statistic is greater than the chi-squared value at the 95% confidence interval.


\newpage

**Question 3 is OPTIONAL.  If you accurately complete it, you will receive 2 bonus points toward your final grade.**

## Question 3:  Estimating Demand via the Multi-Nomial Logit Model (MNL)

Suppose you have $i=1,\ldots,n$ consumers who each select exactly one product $j$ from a set of $J$ products. The outcome variable is the identity of the product chosen $y_i \in \{1, \ldots, J\}$ or equivalently a vector of $J-1$ zeros and $1$ one, where the $1$ indicates the selected product. For example, if the third product was chosen out of 4 products, then either $y=3$ or $y=(0,0,1,0)$ depending on how you want to represent it. Suppose also that you have a vector of data on each product $x_j$ (eg, size, price, etc.). 

The MNL model posits that the probability that consumer $i$ chooses product $j$ is:

$$ \mathbb{P}_i(j) = \frac{e^{x_j'\beta}}{\sum_{k=1}^Je^{x_k'\beta}} $$

For example, if there are 4 products, the probability that consumer $i$ chooses product 3 is:

$$ \mathbb{P}_i(3) = \frac{e^{x_3'\beta}}{e^{x_1'\beta} + e^{x_2'\beta} + e^{x_3'\beta} + e^{x_4'\beta}} $$

A clever way to write the individual likelihood function for consumer $i$ is the product of the $J$ probabilities, each raised to the power of an indicator variable ($\delta_{ij}$) that indicates the chosen product:

$$ L_i(\beta) = \prod_{j=1}^J \mathbb{P}_i(j)^{\delta_{ij}} = \mathbb{P}_i(1)^{\delta_{i1}} \times \ldots \times \mathbb{P}_i(J)^{\delta_{iJ}}$$

Notice that if the consumer selected product $j=3$, then $\delta_{i3}=1$ while $\delta_{i1}=\delta_{i2}=\delta_{i4}=0$ and the likelihood is:

$$ L_i(\beta) = \mathbb{P}_i(1)^0 \times \mathbb{P}_i(2)^0 \times \mathbb{P}_i(3)^1 \times \mathbb{P}_i(4)^0 = \mathbb{P}_i(3) = \frac{e^{x_3'\beta}}{\sum_{k=1}^Je^{x_k'\beta}} $$

The joint likelihood (across all consumers) is the product of the $n$ individual likelihoods:

$$ L_n(\beta) = \prod_{i=1}^n L_i(\beta) = \prod_{i=1}^n \prod_{j=1}^J \mathbb{P}_i(j)^{\delta_{ij}} $$

And the joint log-likelihood function is:

$$ \ell_n(\beta) = \sum_{i=1}^n \sum_{j=1}^J \delta_{ij} \log(\mathbb{P}_i(j)) $$

Use the `yogurt_data` dataset, which provides the anonymized consumer identifiers (`id`), a vector indicating the chosen product (`y1`:`y4`), a vector indicating if any products were "featured" in the store as a form of advertising (`f1`:`f4`), and the products' prices (`p1`:`p4`). For example, consumer 1 purchased yogurt 4 at a price of 0.079/oz and none of the yogurts were featured/advertised at the time of consumer 1's purchase.  Consumers 2 through 7 each bought yogurt 2, etc.

Let the vector of product features include brand dummy variables for yogurts 1-3 (omit a dummy for product 4 to avoid multi-collinearity), a dummy variable to indicate featured, and a continuous variable for price:  

$$ x_j' = [\mathbbm{1}(\text{Yogurt 1}), \mathbbm{1}(\text{Yogurt 2}), \mathbbm{1}(\text{Yogurt 3}), X_f, X_p] $$

You will need to create the product dummies. The variables for featured and price are included in the dataset. The "hard part" of this likelihood function is organizing the data.

Your task: Code up the log-likelihood function.  Use `optim()` to find the MLEs for the 5 parameters ($\beta_1, \beta_2, \beta_3, \beta_f, \beta_p$). 

(Hint: you should find 2 positive and 1 negative product intercepts, a small positive coefficient estimate for featured, and a large negative coefficient estimate for price.)

```{r}
## finished this partly with the help of ChatGPT and partly with my study group.
library(tidyverse)
yogurt_data <- read.csv("yogurt_data.csv")
yogurt_data <- yogurt_data %>% 
  mutate(
    yogurt_1 = as.integer(y1 == 1),
    yogurt_2 = as.integer(y2 == 1),
    yogurt_3 = as.integer(y3 == 1)
  )
log_likelihood <- function(beta, data) {
  x1Beta <- beta[1] + beta[4] * data$f1 + beta[5] * data$p1
  x2Beta <- beta[2] + beta[4] * data$f2 + beta[5] * data$p2
  x3Beta <- beta[3] + beta[4] * data$f3 + beta[5] * data$p3
  x4Beta <- 0 + beta[4] * data$f4 + beta[5] * data$p4
  prob_1 <- exp(x1Beta) / (exp(x1Beta) + exp(x2Beta) + exp(x3Beta) + exp(x4Beta))
  prob_2 <- exp(x2Beta) / (exp(x1Beta) + exp(x2Beta) + exp(x3Beta) + exp(x4Beta))
  prob_3 <- exp(x3Beta) / (exp(x1Beta) + exp(x2Beta) + exp(x3Beta) + exp(x4Beta))
  prob_4 <- exp(x4Beta) / (exp(x1Beta) + exp(x2Beta) + exp(x3Beta) + exp(x4Beta))
  ll <- sum(data$yogurt_1 * log(prob_1) +
              data$yogurt_2 * log(prob_2) +
              data$yogurt_3 * log(prob_3) +
              (1 - data$yogurt_1 - data$yogurt_2 - data$yogurt_3) * log(prob_4))
  return(ll)
} 
out_optim <- optim(par = rep(0, 5), fn = log_likelihood, data = yogurt_data, control = list(fnscale = -1),hessian = TRUE, method="BFGS")
out_optim$par
```

This matches with the hint provided (2 positive and 1 negative product intercepts, a small positive coefficient estimate for featured, and a large negative coefficient estimate for price.)
